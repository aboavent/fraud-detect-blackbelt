{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys;\n",
    "#!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sagemaker-jfox in region eu-west-1\n",
      "CPU times: user 865 ms, sys: 72.5 ms, total: 938 ms\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() \n",
    "region = session.boto_region_name \n",
    "\n",
    "bucket = 'sagemaker-jfox'\n",
    " \n",
    "prefix = 'sagemaker/xgboost'\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Bucket {bucket} in region {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 64124\n",
      "CPU times: user 191 ms, sys: 32.4 ms, total: 224 ms\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>DEBIT</td>\n",
       "      <td>5337.77</td>\n",
       "      <td>C712410124</td>\n",
       "      <td>41720.00</td>\n",
       "      <td>36382.23</td>\n",
       "      <td>C195600860</td>\n",
       "      <td>41898.00</td>\n",
       "      <td>40348.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>55105.90</td>\n",
       "      <td>C2007486296</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C932583850</td>\n",
       "      <td>317952.89</td>\n",
       "      <td>2719172.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64122</th>\n",
       "      <td>726</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>3547010.54</td>\n",
       "      <td>C456762120</td>\n",
       "      <td>3547010.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1618899924</td>\n",
       "      <td>134668.90</td>\n",
       "      <td>3681679.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64123</th>\n",
       "      <td>730</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>10000000.00</td>\n",
       "      <td>C726730575</td>\n",
       "      <td>57316255.05</td>\n",
       "      <td>47316255.05</td>\n",
       "      <td>C1364745638</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64124 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       step      type       amount     nameOrig  oldbalanceOrg  \\\n",
       "0         1     DEBIT      5337.77   C712410124       41720.00   \n",
       "1         1  CASH_OUT     55105.90  C2007486296           0.00   \n",
       "...     ...       ...          ...          ...            ...   \n",
       "64122   726  CASH_OUT   3547010.54   C456762120     3547010.54   \n",
       "64123   730  TRANSFER  10000000.00   C726730575    57316255.05   \n",
       "\n",
       "       newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n",
       "0            36382.23   C195600860        41898.00        40348.79        0   \n",
       "1                0.00   C932583850       317952.89      2719172.89        0   \n",
       "...               ...          ...             ...             ...      ...   \n",
       "64122            0.00  C1618899924       134668.90      3681679.44        1   \n",
       "64123     47316255.05  C1364745638            0.00            0.00        1   \n",
       "\n",
       "       isFlaggedFraud  \n",
       "0                   0  \n",
       "1                   0  \n",
       "...               ...  \n",
       "64122               0  \n",
       "64123               0  \n",
       "\n",
       "[64124 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "import pandas as pd\n",
    "raw_data_filename = 'short-fraud-detection.csv'\n",
    "\n",
    "s3 = boto3.resource('s3', region_name=region)\n",
    "#s3.Bucket(bucket).download_file(raw_data_filename, raw_data_filename)\n",
    "percent_to_read=100\n",
    "fraction_to_read=percent_to_read/100  \n",
    "df = pd.read_csv('./'+raw_data_filename,  skiprows=lambda i: i>0 and random.random() > fraction_to_read)\n",
    "pd.set_option('display.max_rows', 5) \n",
    "\n",
    "print('Length', len(df))\n",
    "target_col='isFraud'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `isFraud` rows have `type` `TRANSFER` or `CASH_OUT`, never but `CASH_IN` or `PAYMENT`. Filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['type'].isin(['TRANSFER', 'CASH_OUT'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of each class to determine imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud 87 ; Not fraud 28040 ; Total 28127\n"
     ]
    }
   ],
   "source": [
    "def count_positive_and_negative(df):\n",
    "    num_positive = len(df.loc[  df[target_col] == 1 ])\n",
    "    num_negative = len(df) - num_positive\n",
    "    return num_positive, num_negative\n",
    "\n",
    "num_positive, num_negative = count_positive_and_negative(df)\n",
    "\n",
    "print('Fraud', num_positive, '; Not fraud', num_negative, '; Total', len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using `isFlaggedFraud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['isFlaggedFraud'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution of positive vs negative, in log scale because of the imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt   \n",
    "def plot_positive_negative_counts(df, target_col):\n",
    "    val_counts=df[target_col].value_counts()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(yscale=\"log\")\n",
    "    plt.bar(['Yes', 'No'], val_counts)\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    "    \n",
    "plot_positive_negative_counts(df,target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine daily cyclical pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "df['step_mod24']=df['step']%24\n",
    "df[\"step_norm\"] = 2 * math.pi * df[\"step\"] / df[\"step\"].max()\n",
    "\n",
    "df['cos_step'] = np.cos(df['step_norm'])\n",
    "df['sin_step'] = np.sin(df['step_norm'])\n",
    "df=df.drop(['step_mod24', 'step_norm'], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference in balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6cb48e97aedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deltaDest'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'newbalanceDest'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oldbalanceDest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'oldbalanceOrg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newBalOrig'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'oldbalanceDest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newbalanceDest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_df' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "#create new columns for difference in Originator ID and Recipient ID\n",
    "df['deltaOrig'] = df['newbalanceOrig'] - df['oldbalanceOrg']\n",
    "df['deltaDest'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    " \n",
    "final_result = result_df.drop(columns = ['oldbalanceOrg', 'newBalOrig', 'oldbalanceDest', 'newbalanceDest'],axis=1)\n",
    "\n",
    "df=df.drop('amount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scale the relevant numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "numerical_cols = ['amount', 'deltaOrig', 'deltaDest' ]\n",
    "other_col =[c for c in columns if  c not in numerical_cols]\n",
    "df_other = df[other_col]\n",
    "\n",
    "df_num = df[numerical_cols]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_num = pd.DataFrame(scaler.fit_transform(df_num), columns=df_num.columns)\n",
    "df = pd.concat([df_num, df_other], axis=1)\n",
    "df = df[columns] # Put back in old order\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dummies (onehot) for `type` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols.remove(target_col)\n",
    "cols = [target_col] + cols\n",
    "df = df[cols] # Move target to the left\n",
    "\n",
    "df_dummies=pd.get_dummies(df['type'],drop_first=True )\n",
    "\n",
    "df=df.drop(['type'], axis=1)\n",
    "df = pd.concat([df, df_dummies], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *HashingEncoder* to handle categorical columns with high cardinality. These cannot be onehotted as that would generate too many columns and a too-sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "high_cardinality__categorical_col = ['nameOrig', 'nameDest']\n",
    "use_hashing_encoder = False\n",
    "if not use_hashing_encoder:\n",
    "    print('Dropping high-cardinality categorical columns')\n",
    "    df=df.drop(high_cardinality__categorical_col, axis=1)\n",
    "else:\n",
    "  import category_encoders as ce  \n",
    "  columns_before = [x for x in df.columns if x not in high_cardinality__categorical_col+[target_col]]\n",
    "\n",
    "  def make_col_mapping(cols):\n",
    "    col_mapping = {}\n",
    "    for c in cols:\n",
    "        if c[:4]=='col_':\n",
    "          num = c.split('_')[-1]\n",
    "          int(num) # check format\n",
    "          col_mapping[c] = hashencode_this + \"_\" + num\n",
    "     \n",
    "    return col_mapping\n",
    "\n",
    "  #TODO hashencode in one pass, both columns\n",
    "  def hashencode(hashencode_this, df, previous_hash_cols):\n",
    "    for c in df.columns:\n",
    "        assert c[:4]!=\"col_\",  df.columns\n",
    "    cpus_in_t2xlarge = 4\n",
    "    default_max_sample = len(df)/cpus_in_t2xlarge\n",
    "    max_sample = default_max_sample/2\n",
    "    ce_hash = ce.HashingEncoder(cols = [hashencode_this],max_sample=max_sample)\n",
    "    X1 = df.drop([target_col], axis=1)\n",
    "    y1 = df[target_col]\n",
    "    with_hashing = ce_hash.fit_transform(X1, y1)\n",
    "    hashed = with_hashing.drop(columns_before+previous_hash_cols, axis=1)\n",
    "    generated_cols = [x for x in hashed.columns if x[:4]=='col_']\n",
    "    col_mapping = make_col_mapping(generated_cols)\n",
    "    \n",
    "    hashed = hashed.rename(columns = col_mapping)\n",
    "    df = pd.concat([y1, X1, hashed], axis=1)\n",
    " \n",
    "    df = df.drop([hashencode_this], axis=1)\n",
    "    return df, list(col_mapping.values())\n",
    " \n",
    "  previous_hash_cols = []\n",
    "  for hashencode_this in high_cardinality__categorical_col: \n",
    "      df, previous_hash_cols = hashencode(hashencode_this,df,previous_hash_cols)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally use *SMOTENC* for unbalanced classes, though we may stick with the XGBoost parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "# Using weighting in XGBOOST instead of SMOTENC\n",
    "use_smote = False\n",
    "if use_smote:\n",
    "   ycol=target_col\n",
    "   Xcol=list(df.columns)\n",
    "   Xcol.remove(ycol)\n",
    " \n",
    "   categorical_columns=[i for i in range(len(Xcol)) \n",
    "                     if Xcol[i] not in ['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']]\n",
    "\n",
    "   smotenc = SMOTENC(categorical_columns,random_state = 101)\n",
    "\n",
    "   X, y = smotenc.fit_resample(df[Xcol], df[ycol])\n",
    "   y_df = pd.DataFrame({target_col: y} )\n",
    "\n",
    "   df = pd.concat([X, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positive_negative_counts(df,target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split  with randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len_=len(df)\n",
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len_), int(0.9 * len_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using libSVM for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import dump_svmlight_file   \n",
    "\n",
    "lengths = [] \n",
    "\n",
    "for d in [(train_data, 'train.libsvm'), ( validation_data, 'validation.libsvm'), (test_data, 'test.libsvm')]:\n",
    "   dataset=d[0]\n",
    "   file_ = d[1]\n",
    "   lengths.append((d[1].split('.')[0],len(dataset)))\n",
    "   dump_svmlight_file(X=dataset.drop([target_col], axis=1), y=dataset[target_col], f=d[1])\n",
    "\n",
    "print('Length of datasets:', lengths )\n",
    "\n",
    "s3 = boto3.resource('s3', region_name=region)#TODO Remove\n",
    "\n",
    "for filename in ['train.libsvm', 'validation.libsvm']:\n",
    "   s3.Bucket(bucket).Object(prefix + '/'+filename.split('.')[0]+'/'+filename).upload_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_path(subset):\n",
    "  return sagemaker.s3_input(s3_data='s3://{}/{}/{}'.format(bucket, prefix,subset), content_type='libsvm')\n",
    "\n",
    "s3_input_train = s3_path('train')\n",
    "s3_input_validation =s3_path('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Our data is now ready to be used to train a XGBoost model. The XGBoost algorithm has many tunable hyperparameters. Some of these hyperparameters are listed below; initially we'll only use a few of them.  \n",
    "\n",
    "- `max_depth`: Maximum depth of a tree. As a cautionary note, a value too small could underfit the data, while increasing it will make the model more complex and thus more likely to overfit the data (in other words, the classic bias-variance tradeoff).\n",
    "- `eta`: Step size shrinkage used in updates to prevent overfitting.  \n",
    "- `eval_metric`: Evaluation metric(s) for validation data. For data sets such as this one with imbalanced classes, we'll use the AUC metric.\n",
    "- `scale_pos_weight`: Controls the balance of positive and negative weights, again useful for data sets having imbalanced classes.\n",
    "\n",
    "First we'll set up the parameters for an Amazon SageMaker Estimator object, and the hyperparameters for the algorithm itself.  The Estimator object from the Amazon SageMaker Python SDK is a convenient way to set up training jobs with a minimal amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(region, 'xgboost','1.0-1')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    base_job_name='fraud-detection-job',\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c5.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=session)\n",
    "scale_pos_weight=num_negative / num_positive  \n",
    "num_positive, num_negative=count_positive_and_negative(df)\n",
    "\n",
    "print('scale_pos_weight', f'{scale_pos_weight:.1f}')\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=3,\n",
    "                        eta=0.1,\n",
    "                        subsample=0.5,\n",
    "                        eval_metric='aucpr',\n",
    "                        objective='binary:logistic',\n",
    "                        scale_pos_weight=scale_pos_weight,\n",
    "                        num_round=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hosted training (`fit`) job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Host\n",
    "\n",
    "Now that we've trained the XGBoost algorithm on our data, we can deploy the trained model to an Amazon SageMaker hosted endpoint with one simple line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "model = xgb.create_model()\n",
    "container_def = model.prepare_container_def(instance_type='ml.m4.xlarge')\n",
    "model_name = 'fraud' + datetime.datetime.now().isoformat().replace('.','-').replace(':','-')  \n",
    "print('model_name', model_name)\n",
    "\n",
    "direct_deploy =true\n",
    "direct_deploy =true\n",
    "endpoint_name='fraud-detection-endpoint'\n",
    "\n",
    "if direct_deploy:\n",
    "  xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n",
    "else:\n",
    "  session.create_model(model_name, role, container_def)\n",
    "\n",
    "  endpoint_config_name = session.create_endpoint_config(name=model_name,\n",
    "                                                      model_name=model_name,\n",
    "                                                      initial_instance_count=1,\n",
    "                                                      instance_type='ml.m5.xlarge')\n",
    "\n",
    "  client = boto3.client('sagemaker')\n",
    "  updated_endpoint=client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "  print(updated_endpoint)\n",
    "  xgb_predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker.Session())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now that we have our hosted endpoint, we can generate predictions from  the  test data set.\n",
    "\n",
    "Compared actual to predicted values of whether the transaction was a \"fraud\" (`1`) or not (`0`).  Then we'll produce a  confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/x-libsvm'\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "def do_predict(data):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = xgb_predictor.predict(payload).decode('utf-8')\n",
    "    result = response.split(',')\n",
    "    preds = [float((num)) for num in result]\n",
    "    preds = [round(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    \n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset+batch_size < items:\n",
    "            results = do_predict(data[offset:(offset+batch_size)])\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items]))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "with open('test.libsvm', 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [int(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = [line for line in payload.split('\\n')]\n",
    "preds = batch_predict(test_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\nError rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))\n",
    "\n",
    "actual_pos = sum(1 for i in range(len(labels)) if 1==labels[i])\n",
    "actual_neg = sum(1 for i in range(len(labels)) if 0==labels[i])\n",
    "\n",
    "pred_pos = sum(1 for i in range(len(preds)) if 1==preds[i])\n",
    "pred_neg = sum(1 for i in range(len(preds)) if 0==preds[i])\n",
    "\n",
    "true_pos = sum(1 for i in range(len(preds)) if  preds[i]==1==labels[i])\n",
    "true_neg = sum(1 for i in range(len(preds)) if  preds[i]==0==labels[i])\n",
    "\n",
    "false_pos=sum(1 for i in range(len(preds)) if  preds[i]==1 and 0==labels[i])\n",
    "false_neg=sum(1 for i in range(len(preds)) if  preds[i]==0 and 1==labels[i])\n",
    "\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "precision = true_pos/(true_pos+false_pos)\n",
    " \n",
    "assert true_pos + false_neg==actual_pos\n",
    "assert true_neg + false_pos==actual_neg\n",
    "#print('pred_pos', pred_pos,'pred_neg', pred_neg)\n",
    "\n",
    "assert len(preds)==len(labels)\n",
    "\n",
    "#print(len(preds), 'predictions and labels')\n",
    "\n",
    "#print('actual_pos', actual_pos, 'actual_neg', actual_neg)\n",
    "assert actual_pos+actual_neg==len(labels)\n",
    "                   \n",
    "#print('true_pos', true_pos, 'false_pos', false_pos)\n",
    "#assert  true_pos+false_pos == pred_pos\n",
    "\n",
    "#print('true_neg', true_neg, 'false_neg', false_neg )\n",
    "assert true_neg+false_neg== pred_neg\n",
    "\n",
    "#print('pred_pos+pred_neg',pred_pos+pred_neg)\n",
    "assert pred_pos+pred_neg==len(preds)\n",
    "\n",
    "print(f'Recall={recall:.2f}')\n",
    "print(f'Precision={precision:.2f}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=np.array(labels), columns=np.array(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
